{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unsupervised learning using pacmac\n",
    "### This file tests combining all networks allowing the gradients from the supervised step to flow all the way back through the previously trained networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\erik-\\anaconda3\\envs\\EL2805\\lib\\site-packages\\torchvision\\io\\image.py:13: UserWarning: Failed to load image Python extension: '[WinError 127] Det gÃ¥r inte att hitta den angivna proceduren'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?\n",
      "  warn(\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torchvision\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from pacmap import PaCMAP\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_preprocess_data():\n",
    "    \"\"\"Loads and preprocesses the MNIST dataset.\"\"\"\n",
    "    train_dataset = torchvision.datasets.MNIST(root='./data', train=True, download=True)\n",
    "    test_dataset = torchvision.datasets.MNIST(root='./data', train=False, download=True)\n",
    "\n",
    "    x_train = train_dataset.data.numpy().astype('float32') / 255.0\n",
    "    y_train = train_dataset.targets.numpy()\n",
    "    x_test = test_dataset.data.numpy().astype('float32') / 255.0\n",
    "    y_test = test_dataset.targets.numpy()\n",
    "\n",
    "    return x_train, y_train, x_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(x_train, y_train, labeled_ratio):\n",
    "    \"\"\"Splits the data into labeled and unlabeled data.\"\"\"\n",
    "    num_labeled = int(labeled_ratio * len(x_train))\n",
    "    x_labeled, x_unlabeled, y_labeled, _ = train_test_split(x_train, y_train, train_size=num_labeled, stratify=y_train, random_state=42)\n",
    "    return x_labeled, x_unlabeled, y_labeled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_pacmap(data, n_components, n_neighbors=10, MN_ratio=0.5, FP_ratio=2.0):\n",
    "    \"\"\"Performs PaCMAP on the data.\"\"\"\n",
    "    pacmap = PaCMAP(n_components=n_components, n_neighbors=n_neighbors, MN_ratio=MN_ratio, FP_ratio=FP_ratio)\n",
    "    return pacmap.fit_transform(data.reshape(data.shape[0], -1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    \"\"\"First neural network model. 28*28 -> 256 -> 128 -> 64.\"\"\"\n",
    "    # do the model below with l2 regularization\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.fc1 = nn.Linear(28 * 28, 256)\n",
    "        self.fc2 = nn.Linear(256, 128)\n",
    "        self.fc3 = nn.Linear(128, 64)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SecondNet(nn.Module):\n",
    "    \"\"\"Second neural network model. 64 -> 32 -> 32 -> 16.\"\"\"\n",
    "    def __init__(self, input_dim=64, hidden_dim=32, output_dim=16):\n",
    "        super(SecondNet, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.fc3 = nn.Linear(hidden_dim, output_dim)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ThirdNet(nn.Module):\n",
    "    \"\"\"Third neural network model. 16 -> 16 -> 16 -> 10.\"\"\"\n",
    "    def __init__(self, input_dim=16, hidden_dim=16, num_classes=10):\n",
    "        super(ThirdNet, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.fc3 = nn.Linear(hidden_dim, num_classes)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CombinedNet1(nn.Module):\n",
    "    def __init__(self, model1, model2):\n",
    "        super(CombinedNet1, self).__init__()\n",
    "        self.model1 = model1\n",
    "        self.model2 = model2\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.model1(x)\n",
    "        x = self.model2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CombinedNet2(nn.Module):\n",
    "    def __init__(self, model1, model2, model3):\n",
    "        super(CombinedNet2, self).__init__()\n",
    "        self.model1 = model1\n",
    "        self.model2 = model2\n",
    "        self.model3 = model3\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.model1(x)\n",
    "        x = self.model2(x)\n",
    "        x = self.model3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, criterion, optimizer, num_epochs, device):\n",
    "    \"\"\"Generic training function for a neural network model.\"\"\"\n",
    "    losses = []\n",
    "    for epoch in tqdm(range(num_epochs)):\n",
    "        model.train()\n",
    "        epoch_loss = 0\n",
    "        for batch_x, batch_y in train_loader:\n",
    "            batch_x, batch_y = batch_x.to(device), batch_y.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(batch_x)\n",
    "            loss = criterion(outputs, batch_y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.item()\n",
    "        losses.append(epoch_loss / len(train_loader))\n",
    "    return losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_first_stage(x_unlabeled, device):\n",
    "    \"\"\"Trains the first neural network.\"\"\"\n",
    "    x_reduced = perform_pacmap(x_unlabeled, n_components=64)\n",
    "    x_train_nn = torch.FloatTensor(x_unlabeled).unsqueeze(1)\n",
    "    y_train_nn = torch.FloatTensor(x_reduced)\n",
    "    train_dataset = TensorDataset(x_train_nn, y_train_nn)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "    model1 = Net().to(device)\n",
    "    criterion1 = nn.MSELoss()\n",
    "    optimizer1 = optim.Adam(model1.parameters())\n",
    "    losses1 = train_model(model1, train_loader, criterion1, optimizer1, num_epochs=10, device=device)\n",
    "    \n",
    "    return model1, x_train_nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_second_stage(model1, x_train_nn, device):\n",
    "    \"\"\"Trains the second neural network.\"\"\"\n",
    "    transformed_unlabeled = model1(x_train_nn.to(device)).detach().cpu().numpy()\n",
    "    x_transformed_16 = perform_pacmap(transformed_unlabeled, n_components=16)\n",
    "    x_train_2 = torch.FloatTensor(transformed_unlabeled)\n",
    "    y_train_2 = torch.FloatTensor(x_transformed_16)\n",
    "    train_dataset_2 = TensorDataset(x_train_2, y_train_2)\n",
    "    train_loader_2 = DataLoader(train_dataset_2, batch_size=32, shuffle=True)\n",
    "\n",
    "    model2 = SecondNet().to(device)\n",
    "    criterion2 = nn.MSELoss()\n",
    "    optimizer2 = optim.Adam(model2.parameters())\n",
    "    losses2 = train_model(model2, train_loader_2, criterion2, optimizer2, num_epochs=10, device=device)\n",
    "    \n",
    "    return model2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_third_stage(model1, model2, x_labeled, y_labeled, device):\n",
    "    \"\"\"Trains the third neural network.\"\"\"\n",
    "    x_labeled_tensor = torch.FloatTensor(x_labeled).unsqueeze(1)\n",
    "    with torch.no_grad():\n",
    "        model1.eval()\n",
    "        model2.eval()\n",
    "        intermediate = model1(x_labeled_tensor.to(device))\n",
    "        processed_labeled = model2(intermediate).cpu().numpy()\n",
    "\n",
    "    x_train_3 = torch.FloatTensor(processed_labeled)\n",
    "    y_train_3 = torch.LongTensor(y_labeled)\n",
    "    train_dataset_3 = TensorDataset(x_train_3, y_train_3)\n",
    "    train_loader_3 = DataLoader(train_dataset_3, batch_size=32, shuffle=True)\n",
    "\n",
    "    model3 = ThirdNet().to(device)\n",
    "    criterion3 = nn.CrossEntropyLoss()\n",
    "    optimizer3 = optim.Adam(model3.parameters())\n",
    "    losses3 = train_model(model3, train_loader_3, criterion3, optimizer3, num_epochs=10, device=device)\n",
    "    \n",
    "    return model3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_combined_model1(model1, model2, x_train_nn, device):\n",
    "    \"\"\"Trains the combined model 1.\"\"\"\n",
    "    combined_model = CombinedNet1(model1, model2).to(device)\n",
    "\n",
    "    x_train_combined = torch.FloatTensor(x_train_nn)\n",
    "    model1_output = model1(x_train_nn.to(device)).detach().cpu().numpy()\n",
    "    y_train_combined = perform_pacmap(model1_output, n_components=16)\n",
    "    y_train_combined = torch.FloatTensor(y_train_combined)\n",
    "\n",
    "    train_dataset_combined = TensorDataset(x_train_combined, y_train_combined)\n",
    "    train_loader_combined = DataLoader(train_dataset_combined, batch_size=32, shuffle=True)\n",
    "\n",
    "    criterion_combined = nn.MSELoss()\n",
    "    optimizer_combined = optim.Adam(combined_model.parameters())\n",
    "\n",
    "    losses_combined = train_model(combined_model, train_loader_combined, criterion_combined, optimizer_combined, num_epochs=10, device=device)\n",
    "\n",
    "    return combined_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_combined_model2(model1, model2, model3, x_labeled, y_labeled, device, num_epochs=10):\n",
    "    combined_model = CombinedNet2(model1, model2, model3).to(device)\n",
    "    \n",
    "    x_labeled_tensor = torch.FloatTensor(x_labeled).unsqueeze(1).to(device)\n",
    "    y_labeled_tensor = torch.LongTensor(y_labeled).to(device)\n",
    "    \n",
    "    train_dataset = TensorDataset(x_labeled_tensor, y_labeled_tensor)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(combined_model.parameters())\n",
    "\n",
    "    losses = []\n",
    "    for epoch in tqdm(range(num_epochs)):\n",
    "        combined_model.train()\n",
    "        epoch_loss = 0\n",
    "        for batch_x, batch_y in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = combined_model(batch_x)\n",
    "            loss = criterion(outputs, batch_y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.item()\n",
    "        losses.append(epoch_loss / len(train_loader))\n",
    "        # print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {losses[-1]:.4f}\")\n",
    "\n",
    "    return combined_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_and_classify(x_new, combined_model, device):\n",
    "    with torch.no_grad():\n",
    "        combined_model.eval()\n",
    "        x_new_tensor = torch.FloatTensor(x_new).unsqueeze(1).to(device)\n",
    "        output = combined_model(x_new_tensor)\n",
    "        _, predicted = output.max(1)\n",
    "    return predicted.cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_combined_model(combined_model, x_test, y_test, device):\n",
    "    predicted_classes = process_and_classify(x_test, combined_model, device)\n",
    "    accuracy = np.mean(predicted_classes == y_test)\n",
    "    print(f\"Accuracy on the test set: {accuracy:.2f}\")\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_evaluate(x_labeled, y_labeled, x_unlabeled, x_test, y_test):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    \n",
    "    # First stage: PaCMAP data to 64 dimensions, then train a NN with embeddings as targets\n",
    "    model1, x_train_nn = train_first_stage(x_unlabeled, device)\n",
    "\n",
    "    # Second stage: Train a second NN with the unlabeled data as input and 16 dimensions as output\n",
    "    model2 = SecondNet().to(device)\n",
    "\n",
    "    # Combined training of the first two models\n",
    "    combined_model = train_combined_model1(model1, model2, x_train_nn, device)\n",
    "\n",
    "    # Third stage: Initialize the third model\n",
    "    model3 = ThirdNet().to(device)\n",
    "\n",
    "    # Combined training of all three models\n",
    "    combined_model = train_combined_model2(model1, model2, model3, x_labeled, y_labeled, device)\n",
    "\n",
    "    # Evaluate on test set\n",
    "    accuracy = evaluate_combined_model(combined_model, x_test, y_test, device)\n",
    "    \n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_evaluate_with_labeled_ratio(labeled_ratio):\n",
    "    \"\"\"Trains and evaluates the model given a ratio of labeled data.\"\"\"\n",
    "    x_train, y_train, x_test, y_test = load_and_preprocess_data()\n",
    "    x_labeled, x_unlabeled, y_labeled = split_data(x_train, y_train, labeled_ratio)\n",
    "    accuracy = train_and_evaluate(x_labeled, y_labeled, x_unlabeled, x_test, y_test)\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training with 50.0% labeled data:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|ââââââââââ| 10/10 [01:04<00:00,  6.44s/it]\n",
      "100%|ââââââââââ| 10/10 [01:02<00:00,  6.22s/it]\n",
      "100%|ââââââââââ| 10/10 [01:10<00:00,  7.06s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on the test set: 0.97\n",
      "\n",
      "Training with 10.0% labeled data:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|ââââââââââ| 10/10 [01:47<00:00, 10.75s/it]\n",
      "100%|ââââââââââ| 10/10 [01:53<00:00, 11.40s/it]\n",
      "100%|ââââââââââ| 10/10 [00:13<00:00,  1.40s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on the test set: 0.96\n",
      "\n",
      "Training with 5.0% labeled data:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|ââââââââââ| 10/10 [01:55<00:00, 11.52s/it]\n",
      "100%|ââââââââââ| 10/10 [02:01<00:00, 12.16s/it]\n",
      "100%|ââââââââââ| 10/10 [00:07<00:00,  1.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on the test set: 0.96\n",
      "\n",
      "Training with 1.0% labeled data:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|ââââââââââ| 10/10 [01:59<00:00, 11.94s/it]\n",
      "100%|ââââââââââ| 10/10 [02:09<00:00, 12.94s/it]\n",
      "100%|ââââââââââ| 10/10 [00:01<00:00,  6.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on the test set: 0.95\n",
      "\n",
      "Training with 0.1% labeled data:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|ââââââââââ| 10/10 [02:04<00:00, 12.43s/it]\n",
      "100%|ââââââââââ| 10/10 [02:04<00:00, 12.46s/it]\n",
      "100%|ââââââââââ| 10/10 [00:00<00:00, 54.88it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on the test set: 0.61\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "labeled_ratios = [0.5, 0.1, 0.05, 0.01, 0.001]\n",
    "accuracies = {}\n",
    "\n",
    "for ratio in labeled_ratios:\n",
    "    print(f\"\\nTraining with {ratio*100}% labeled data:\")\n",
    "    accuracy = train_and_evaluate_with_labeled_ratio(ratio)\n",
    "    accuracies[ratio] = accuracy\n",
    "labeled_ratios = [0.5, 0.1, 0.05, 0.01, 0.001]\n",
    "accuracies_val = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training with one sample per class:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|ââââââââââ| 10/10 [01:57<00:00, 11.72s/it]\n",
      "100%|ââââââââââ| 10/10 [02:09<00:00, 12.90s/it]\n",
      "100%|ââââââââââ| 10/10 [00:00<00:00, 119.43it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on the test set: 0.27\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "x_train, y_train, x_test, y_test = load_and_preprocess_data()\n",
    "\n",
    "# One sample per class\n",
    "x_labeled_one_per_class = []\n",
    "y_labeled_one_per_class = []\n",
    "x_unlabeled_one_per_class = []\n",
    "for i in range(10):\n",
    "    indices = np.where(y_train == i)[0]\n",
    "    x_labeled_one_per_class.append(x_train[indices[0]])\n",
    "    y_labeled_one_per_class.append(i)\n",
    "    x_unlabeled_one_per_class.extend(x_train[indices[1:]])\n",
    "\n",
    "x_labeled_one_per_class = np.array(x_labeled_one_per_class)\n",
    "y_labeled_one_per_class = np.array(y_labeled_one_per_class)\n",
    "x_unlabeled_one_per_class = np.array(x_unlabeled_one_per_class)\n",
    "\n",
    "print(\"\\nTraining with one sample per class:\")\n",
    "accuracy_one_per_class = train_and_evaluate(x_labeled_one_per_class, y_labeled_one_per_class, x_unlabeled_one_per_class, x_test, y_test)\n",
    "accuracies['one_per_class'] = accuracy_one_per_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy with 50.0% labeled data: 0.9735\n",
      "Accuracy with 10.0% labeled data: 0.9613\n",
      "Accuracy with 5.0% labeled data: 0.9593\n",
      "Accuracy with 1.0% labeled data: 0.9513\n",
      "Accuracy with 0.1% labeled data: 0.6117\n",
      "Accuracy with one sample per class: 0.2667\n"
     ]
    }
   ],
   "source": [
    "# Print results\n",
    "for ratio, accuracy in accuracies.items():\n",
    "    if ratio == 'one_per_class':\n",
    "        print(f\"Accuracy with one sample per class: {accuracy}\")\n",
    "    else:\n",
    "        print(f\"Accuracy with {ratio*100}% labeled data: {accuracy}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "EL2805",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
