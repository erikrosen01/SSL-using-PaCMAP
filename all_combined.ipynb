{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unsupervised learning using pacmac\n",
    "### This file tests combining all networks allowing the gradients from the supervised step to flow all the way back through the previously trained networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torchvision\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from pacmap import PaCMAP\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_preprocess_data():\n",
    "    \"\"\"Loads and preprocesses the MNIST dataset.\"\"\"\n",
    "    train_dataset = torchvision.datasets.MNIST(root='./data', train=True, download=True)\n",
    "    test_dataset = torchvision.datasets.MNIST(root='./data', train=False, download=True)\n",
    "\n",
    "    x_train = train_dataset.data.numpy().astype('float32') / 255.0\n",
    "    y_train = train_dataset.targets.numpy()\n",
    "    x_test = test_dataset.data.numpy().astype('float32') / 255.0\n",
    "    y_test = test_dataset.targets.numpy()\n",
    "\n",
    "    return x_train, y_train, x_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(x_train, y_train, labeled_ratio):\n",
    "    \"\"\"Splits the data into labeled and unlabeled data.\"\"\"\n",
    "    num_labeled = int(labeled_ratio * len(x_train))\n",
    "    x_labeled, x_unlabeled, y_labeled, _ = train_test_split(x_train, y_train, train_size=num_labeled, stratify=y_train, random_state=42)\n",
    "    return x_labeled, x_unlabeled, y_labeled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_pacmap(data, n_components, n_neighbors=10, MN_ratio=0.5, FP_ratio=2.0):\n",
    "    \"\"\"Performs PaCMAP on the data.\"\"\"\n",
    "    pacmap = PaCMAP(n_components=n_components, n_neighbors=n_neighbors, MN_ratio=MN_ratio, FP_ratio=FP_ratio)\n",
    "    return pacmap.fit_transform(data.reshape(data.shape[0], -1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    \"\"\"First neural network model. 28*28 -> 256 -> 128 -> 64.\"\"\"\n",
    "    # do the model below with l2 regularization\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.fc1 = nn.Linear(28 * 28, 256)\n",
    "        self.fc2 = nn.Linear(256, 128)\n",
    "        self.fc3 = nn.Linear(128, 64)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SecondNet(nn.Module):\n",
    "    \"\"\"Second neural network model. 64 -> 32 -> 32 -> 16.\"\"\"\n",
    "    def __init__(self, input_dim=64, hidden_dim=32, output_dim=16):\n",
    "        super(SecondNet, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.fc3 = nn.Linear(hidden_dim, output_dim)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ThirdNet(nn.Module):\n",
    "    \"\"\"Third neural network model. 16 -> 16 -> 16 -> 10.\"\"\"\n",
    "    def __init__(self, input_dim=16, hidden_dim=16, num_classes=10):\n",
    "        super(ThirdNet, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.fc3 = nn.Linear(hidden_dim, num_classes)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CombinedNet(nn.Module):\n",
    "    def __init__(self, model1, model2, model3):\n",
    "        super(CombinedNet, self).__init__()\n",
    "        self.model1 = model1\n",
    "        self.model2 = model2\n",
    "        self.model3 = model3\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.model1(x)\n",
    "        x = self.model2(x)\n",
    "        x = self.model3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, criterion, optimizer, num_epochs, device):\n",
    "    \"\"\"Generic training function for a neural network model.\"\"\"\n",
    "    losses = []\n",
    "    for epoch in tqdm(range(num_epochs)):\n",
    "        model.train()\n",
    "        epoch_loss = 0\n",
    "        for batch_x, batch_y in train_loader:\n",
    "            batch_x, batch_y = batch_x.to(device), batch_y.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(batch_x)\n",
    "            loss = criterion(outputs, batch_y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.item()\n",
    "        losses.append(epoch_loss / len(train_loader))\n",
    "    return losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_first_stage(x_unlabeled, x_labeled, device):\n",
    "    \"\"\"Trains the first neural network.\"\"\"\n",
    "    x_total = np.concatenate([x_unlabeled, x_labeled])\n",
    "    x_reduced = perform_pacmap(x_total, n_components=64)\n",
    "    x_train_nn = torch.FloatTensor(x_total).unsqueeze(1)\n",
    "    y_train_nn = torch.FloatTensor(x_reduced)\n",
    "    train_dataset = TensorDataset(x_train_nn, y_train_nn)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "    model1 = Net().to(device)\n",
    "    criterion1 = nn.MSELoss()\n",
    "    optimizer1 = optim.Adam(model1.parameters())\n",
    "    losses1 = train_model(model1, train_loader, criterion1, optimizer1, num_epochs=10, device=device)\n",
    "    \n",
    "    return model1, x_train_nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_second_stage(model1, x_train_nn, device):\n",
    "    \"\"\"Trains the second neural network.\"\"\"\n",
    "    transformed_unlabeled = model1(x_train_nn.to(device)).detach().cpu().numpy()\n",
    "    x_transformed_16 = perform_pacmap(transformed_unlabeled, n_components=16)\n",
    "    x_train_2 = torch.FloatTensor(transformed_unlabeled)\n",
    "    y_train_2 = torch.FloatTensor(x_transformed_16)\n",
    "    train_dataset_2 = TensorDataset(x_train_2, y_train_2)\n",
    "    train_loader_2 = DataLoader(train_dataset_2, batch_size=32, shuffle=True)\n",
    "\n",
    "    model2 = SecondNet().to(device)\n",
    "    criterion2 = nn.MSELoss()\n",
    "    optimizer2 = optim.Adam(model2.parameters())\n",
    "    losses2 = train_model(model2, train_loader_2, criterion2, optimizer2, num_epochs=10, device=device)\n",
    "    \n",
    "    return model2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_third_stage(model1, model2, x_labeled, y_labeled, device):\n",
    "    \"\"\"Trains the third neural network.\"\"\"\n",
    "    x_labeled_tensor = torch.FloatTensor(x_labeled).unsqueeze(1)\n",
    "    with torch.no_grad():\n",
    "        model1.eval()\n",
    "        model2.eval()\n",
    "        intermediate = model1(x_labeled_tensor.to(device))\n",
    "        processed_labeled = model2(intermediate).cpu().numpy()\n",
    "\n",
    "    x_train_3 = torch.FloatTensor(processed_labeled)\n",
    "    y_train_3 = torch.LongTensor(y_labeled)\n",
    "    train_dataset_3 = TensorDataset(x_train_3, y_train_3)\n",
    "    train_loader_3 = DataLoader(train_dataset_3, batch_size=32, shuffle=True)\n",
    "\n",
    "    model3 = ThirdNet().to(device)\n",
    "    criterion3 = nn.CrossEntropyLoss()\n",
    "    optimizer3 = optim.Adam(model3.parameters())\n",
    "    losses3 = train_model(model3, train_loader_3, criterion3, optimizer3, num_epochs=10, device=device)\n",
    "    \n",
    "    return model3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_combined_model(model1, model2, model3, x_labeled, y_labeled, device, num_epochs=10):\n",
    "    combined_model = CombinedNet(model1, model2, model3).to(device)\n",
    "    \n",
    "    x_labeled_tensor = torch.FloatTensor(x_labeled).unsqueeze(1).to(device)\n",
    "    y_labeled_tensor = torch.LongTensor(y_labeled).to(device)\n",
    "    \n",
    "    train_dataset = TensorDataset(x_labeled_tensor, y_labeled_tensor)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(combined_model.parameters())\n",
    "\n",
    "    losses = []\n",
    "    for epoch in tqdm(range(num_epochs)):\n",
    "        combined_model.train()\n",
    "        epoch_loss = 0\n",
    "        for batch_x, batch_y in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = combined_model(batch_x)\n",
    "            loss = criterion(outputs, batch_y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.item()\n",
    "        losses.append(epoch_loss / len(train_loader))\n",
    "        # print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {losses[-1]:.4f}\")\n",
    "\n",
    "    return combined_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_first_stage_early_stopping(x_unlabeled, device, val_frac=0.1):\n",
    "    \"\"\"Trains the first neural network with early stopping.\"\"\"\n",
    "    x_reduced = perform_pacmap(x_unlabeled, n_components=64)\n",
    "    val_len = int(val_frac * len(x_reduced))\n",
    "    x_train_nn = torch.FloatTensor(x_unlabeled).unsqueeze(1)\n",
    "    y_train_nn = torch.FloatTensor(x_reduced)\n",
    "    train_dataset = TensorDataset(x_train_nn[:-val_len], y_train_nn[:-val_len])\n",
    "    val_dataset = TensorDataset(x_train_nn[-val_len:], y_train_nn[-val_len:])\n",
    "    train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "    model1 = Net().to(device)\n",
    "    criterion1 = nn.MSELoss()\n",
    "    optimizer1 = optim.Adam(model1.parameters())\n",
    "    best_loss = np.inf\n",
    "    best_model = None\n",
    "\n",
    "    for epoch in tqdm(range(20)):\n",
    "        model1.train()\n",
    "        epoch_loss = 0\n",
    "        for batch_x, batch_y in train_loader:\n",
    "            batch_x, batch_y = batch_x.to(device), batch_y.to(device)\n",
    "            optimizer1.zero_grad()\n",
    "            outputs = model1(batch_x)\n",
    "            loss = criterion1(outputs, batch_y)\n",
    "            loss.backward()\n",
    "            optimizer1.step()\n",
    "            epoch_loss += loss.item()\n",
    "        val_loss = 0\n",
    "        for batch_x, batch_y in val_loader:\n",
    "            batch_x, batch_y = batch_x.to(device), batch_y.to(device)\n",
    "            outputs = model1(batch_x)\n",
    "            loss = criterion1(outputs, batch_y)\n",
    "            val_loss += loss.item()\n",
    "        if val_loss < best_loss:\n",
    "            best_loss = val_loss\n",
    "            best_model = model1.state_dict()\n",
    "        else:\n",
    "            break\n",
    "    model1.load_state_dict(best_model)\n",
    "    \n",
    "    return model1, x_train_nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_second_stage_early_stopping(model1, x_train_nn, device, val_frac=0.1):\n",
    "    \"\"\"Trains the second neural network with early stopping.\"\"\"\n",
    "    transformed_unlabeled = model1(x_train_nn.to(device)).detach().cpu().numpy()\n",
    "    x_transformed_16 = perform_pacmap(transformed_unlabeled, n_components=16)\n",
    "    val_len = int(val_frac * len(x_transformed_16))\n",
    "    x_train_2 = torch.FloatTensor(transformed_unlabeled)\n",
    "    y_train_2 = torch.FloatTensor(x_transformed_16)\n",
    "    train_dataset_2 = TensorDataset(x_train_2[:-val_len], y_train_2[:-val_len])\n",
    "    val_dataset_2 = TensorDataset(x_train_2[-val_len:], y_train_2[-val_len:])\n",
    "    train_loader_2 = DataLoader(train_dataset_2, batch_size=32, shuffle=True)\n",
    "    val_loader_2 = DataLoader(val_dataset_2, batch_size=32, shuffle=False)\n",
    "\n",
    "    model2 = SecondNet().to(device)\n",
    "    criterion2 = nn.MSELoss()\n",
    "    optimizer2 = optim.Adam(model2.parameters())\n",
    "    best_loss = np.inf\n",
    "    best_model = None\n",
    "\n",
    "    for epoch in tqdm(range(20)):\n",
    "        model2.train()\n",
    "        epoch_loss = 0\n",
    "        for batch_x, batch_y in train_loader_2:\n",
    "            batch_x, batch_y = batch_x.to(device), batch_y.to(device)\n",
    "            optimizer2.zero_grad()\n",
    "            outputs = model2(batch_x)\n",
    "            loss = criterion2(outputs, batch_y)\n",
    "            loss.backward()\n",
    "            optimizer2.step()\n",
    "            epoch_loss += loss.item()\n",
    "        val_loss = 0\n",
    "        for batch_x, batch_y in val_loader_2:\n",
    "            batch_x, batch_y = batch_x.to(device), batch_y.to(device)\n",
    "            outputs = model2(batch_x)\n",
    "            loss = criterion2(outputs, batch_y)\n",
    "            val_loss += loss.item()\n",
    "        if val_loss < best_loss:\n",
    "            best_loss = val_loss\n",
    "            best_model = model2.state_dict()\n",
    "        else:\n",
    "            break\n",
    "    model2.load_state_dict(best_model)\n",
    "\n",
    "    return model2    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_third_stage_early_stopping(model1, model2, x_labeled, y_labeled, device, val_frac=0.1):\n",
    "    \"\"\"Trains the third neural network with early stopping.\"\"\"\n",
    "    if len(x_labeled) <= 10:\n",
    "        return train_third_stage(model1, model2, x_labeled, y_labeled, device)\n",
    "    \n",
    "    val_len = int(val_frac * len(x_labeled))\n",
    "    x_labeled_tensor = torch.FloatTensor(x_labeled).unsqueeze(1)\n",
    "    with torch.no_grad():\n",
    "        model1.eval()\n",
    "        model2.eval()\n",
    "        intermediate = model1(x_labeled_tensor.to(device))\n",
    "        processed_labeled = model2(intermediate).cpu().numpy()\n",
    "\n",
    "    x_train_3 = torch.FloatTensor(processed_labeled)\n",
    "    y_train_3 = torch.LongTensor(y_labeled)\n",
    "    train_dataset_3 = TensorDataset(x_train_3[:-val_len], y_train_3[:-val_len])\n",
    "    val_dataset_3 = TensorDataset(x_train_3[-val_len:], y_train_3[-val_len:])\n",
    "    train_loader_3 = DataLoader(train_dataset_3, batch_size=32, shuffle=True)\n",
    "    val_loader_3 = DataLoader(val_dataset_3, batch_size=32, shuffle=False)\n",
    "\n",
    "    model3 = ThirdNet().to(device)\n",
    "    criterion3 = nn.CrossEntropyLoss()\n",
    "    optimizer3 = optim.Adam(model3.parameters())\n",
    "    best_loss = np.inf\n",
    "    best_model = None\n",
    "\n",
    "    for epoch in tqdm(range(20)):\n",
    "        model3.train()\n",
    "        epoch_loss = 0\n",
    "        for batch_x, batch_y in train_loader_3:\n",
    "            batch_x, batch_y = batch_x.to(device), batch_y.to(device)\n",
    "            optimizer3.zero_grad()\n",
    "            outputs = model3(batch_x)\n",
    "            loss = criterion3(outputs, batch_y)\n",
    "            loss.backward()\n",
    "            optimizer3.step()\n",
    "            epoch_loss += loss.item()\n",
    "        val_loss = 0\n",
    "        for batch_x, batch_y in val_loader_3:\n",
    "            batch_x, batch_y = batch_x.to(device), batch_y.to(device)\n",
    "            outputs = model3(batch_x)\n",
    "            loss = criterion3(outputs, batch_y)\n",
    "            val_loss += loss.item()\n",
    "        if val_loss < best_loss:\n",
    "            best_loss = val_loss\n",
    "            best_model = model3.state_dict()\n",
    "        else:\n",
    "            break\n",
    "    model3.load_state_dict(best_model)\n",
    "\n",
    "    return model3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_combined_model_early_stopping(model1, model2, model3, x_labeled, y_labeled, device, num_epochs=10, val_frac=0.1):\n",
    "    combined_model = CombinedNet(model1, model2, model3).to(device)\n",
    "\n",
    "    if len(x_labeled) <= 10:\n",
    "        return train_combined_model(model1, model2, model3, x_labeled, y_labeled, device, num_epochs)\n",
    "    \n",
    "    x_labeled_tensor = torch.FloatTensor(x_labeled).unsqueeze(1).to(device)\n",
    "    y_labeled_tensor = torch.LongTensor(y_labeled).to(device)\n",
    "    \n",
    "    train_dataset = TensorDataset(x_labeled_tensor[:-int(val_frac * len(x_labeled))], y_labeled_tensor[:-int(val_frac * len(x_labeled))])\n",
    "    val_dataset = TensorDataset(x_labeled_tensor[-int(val_frac * len(x_labeled)):], y_labeled_tensor[-int(val_frac * len(x_labeled)):])\n",
    "    train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(combined_model.parameters())\n",
    "    best_loss = np.inf\n",
    "    best_model = None\n",
    "\n",
    "    for epoch in tqdm(range(20)):\n",
    "        combined_model.train()\n",
    "        epoch_loss = 0\n",
    "        for batch_x, batch_y in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = combined_model(batch_x)\n",
    "            loss = criterion(outputs, batch_y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.item()\n",
    "        val_loss = 0\n",
    "        for batch_x, batch_y in val_loader:\n",
    "            outputs = combined_model(batch_x)\n",
    "            loss = criterion(outputs, batch_y)\n",
    "            val_loss += loss.item()\n",
    "        if val_loss < best_loss:\n",
    "            best_loss = val_loss\n",
    "            best_model = combined_model.state_dict()\n",
    "        else:\n",
    "            break\n",
    "    combined_model.load_state_dict(best_model)\n",
    "\n",
    "    return combined_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_and_classify(x_new, combined_model, device):\n",
    "    with torch.no_grad():\n",
    "        combined_model.eval()\n",
    "        x_new_tensor = torch.FloatTensor(x_new).unsqueeze(1).to(device)\n",
    "        output = combined_model(x_new_tensor)\n",
    "        _, predicted = output.max(1)\n",
    "    return predicted.cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_combined_model(combined_model, x_test, y_test, device):\n",
    "    predicted_classes = process_and_classify(x_test, combined_model, device)\n",
    "    accuracy = np.mean(predicted_classes == y_test)\n",
    "    print(f\"Accuracy on the test set: {accuracy:.2f}\")\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_evaluate(x_labeled, y_labeled, x_unlabeled, x_test, y_test):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    \n",
    "    # First stage: PaCMAP data to 64 dimensions, then train a NN with embeddings as targets\n",
    "    model1, x_train_nn = train_first_stage(x_unlabeled, x_labeled, device)\n",
    "\n",
    "    # Second stage: PaCMAP the output of the first NN to 16 dimensions, then train a NN with embeddings as targets\n",
    "    model2 = train_second_stage(model1, x_train_nn, device)\n",
    "\n",
    "    # Third stage: Initialize the third model\n",
    "    model3 = ThirdNet().to(device)\n",
    "\n",
    "    # Combined training of all three models\n",
    "    combined_model = train_combined_model(model1, model2, model3, x_labeled, y_labeled, device)\n",
    "\n",
    "    # Evaluate on test set\n",
    "    accuracy = evaluate_combined_model(combined_model, x_test, y_test, device)\n",
    "    \n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_evaluate_early_stopping(x_labeled, y_labeled, x_unlabeled, x_test, y_test):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    \n",
    "    # First stage: PaCMAP data to 64 dimensions, then train a NN with embeddings as targets\n",
    "    model1, x_train_nn = train_first_stage_early_stopping(x_unlabeled, device)\n",
    "\n",
    "    # Second stage: PaCMAP the output of the first NN to 16 dimensions, then train a NN with embeddings as targets\n",
    "    model2 = train_second_stage_early_stopping(model1, x_train_nn, device)\n",
    "\n",
    "    # Third stage: Initialize the third model\n",
    "    model3 = ThirdNet().to(device)\n",
    "\n",
    "    # Combined training of all three models\n",
    "    combined_model = train_combined_model_early_stopping(model1, model2, model3, x_labeled, y_labeled, device)\n",
    "\n",
    "    # Evaluate on test set\n",
    "    accuracy = evaluate_combined_model(combined_model, x_test, y_test, device)\n",
    "\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_evaluate_with_labeled_ratio(labeled_ratio):\n",
    "    \"\"\"Trains and evaluates the model given a ratio of labeled data.\"\"\"\n",
    "    x_train, y_train, x_test, y_test = load_and_preprocess_data()\n",
    "    x_labeled, x_unlabeled, y_labeled = split_data(x_train, y_train, labeled_ratio)\n",
    "    accuracy = train_and_evaluate(x_labeled, y_labeled, x_unlabeled, x_test, y_test)\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_evaluate_with_labeled_ratio_early_stopping(labeled_ratio):\n",
    "    \"\"\"Trains and evaluates the model given a ratio of labeled data with early stopping.\"\"\"\n",
    "    x_train, y_train, x_test, y_test = load_and_preprocess_data()\n",
    "    x_labeled, x_unlabeled, y_labeled = split_data(x_train, y_train, labeled_ratio)\n",
    "    accuracy = train_and_evaluate_early_stopping(x_labeled, y_labeled, x_unlabeled, x_test, y_test)\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training with 50.0% labeled data:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 10/20 [01:01<01:01,  6.19s/it]\n",
      " 95%|█████████▌| 19/20 [00:30<00:01,  1.63s/it]\n",
      " 15%|█▌        | 3/20 [00:27<02:37,  9.25s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on the test set: 0.97\n",
      "\n",
      "Training with 10.0% labeled data:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 35%|███▌      | 7/20 [01:29<02:45, 12.72s/it]\n",
      " 90%|█████████ | 18/20 [00:56<00:06,  3.13s/it]\n",
      " 15%|█▌        | 3/20 [00:05<00:32,  1.89s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on the test set: 0.96\n",
      "\n",
      "Training with 5.0% labeled data:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|██▌       | 5/20 [01:06<03:18, 13.21s/it]\n",
      " 95%|█████████▌| 19/20 [01:07<00:03,  3.54s/it]\n",
      " 20%|██        | 4/20 [00:03<00:14,  1.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on the test set: 0.95\n",
      "\n",
      "Training with 1.0% labeled data:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 6/20 [01:19<03:05, 13.28s/it]\n",
      " 70%|███████   | 14/20 [00:48<00:20,  3.43s/it]\n",
      " 25%|██▌       | 5/20 [00:00<00:02,  6.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on the test set: 0.94\n",
      "\n",
      "Training with 0.1% labeled data:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 6/20 [01:24<03:18, 14.17s/it]\n",
      " 50%|█████     | 10/20 [00:34<00:34,  3.46s/it]\n",
      " 65%|██████▌   | 13/20 [00:00<00:00, 53.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on the test set: 0.68\n"
     ]
    }
   ],
   "source": [
    "labeled_ratios = [0.5, 0.1, 0.05, 0.01, 0.001]\n",
    "accuracies = {}\n",
    "\n",
    "for ratio in labeled_ratios:\n",
    "    print(f\"\\nTraining with {ratio*100}% labeled data:\")\n",
    "    accuracy = train_and_evaluate_with_labeled_ratio_early_stopping(ratio)\n",
    "    accuracies[ratio] = accuracy\n",
    "labeled_ratios = [0.5, 0.1, 0.05, 0.01, 0.001]\n",
    "accuracies_val = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training with one sample per class:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 4/20 [00:55<03:40, 13.76s/it]\n",
      " 65%|██████▌   | 13/20 [00:42<00:22,  3.28s/it]\n",
      "100%|██████████| 10/10 [00:00<00:00, 134.96it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on the test set: 0.28\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "x_train, y_train, x_test, y_test = load_and_preprocess_data()\n",
    "\n",
    "# One sample per class\n",
    "x_labeled_one_per_class = []\n",
    "y_labeled_one_per_class = []\n",
    "x_unlabeled_one_per_class = []\n",
    "for i in range(10):\n",
    "    indices = np.where(y_train == i)[0]\n",
    "    x_labeled_one_per_class.append(x_train[indices[0]])\n",
    "    y_labeled_one_per_class.append(i)\n",
    "    x_unlabeled_one_per_class.extend(x_train[indices[1:]])\n",
    "\n",
    "x_labeled_one_per_class = np.array(x_labeled_one_per_class)\n",
    "y_labeled_one_per_class = np.array(y_labeled_one_per_class)\n",
    "x_unlabeled_one_per_class = np.array(x_unlabeled_one_per_class)\n",
    "\n",
    "print(\"\\nTraining with one sample per class:\")\n",
    "accuracy_one_per_class = train_and_evaluate_early_stopping(x_labeled_one_per_class, y_labeled_one_per_class, x_unlabeled_one_per_class, x_test, y_test)\n",
    "accuracies['one_per_class'] = accuracy_one_per_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy with 50.0% labeled data: 0.9684\n",
      "Accuracy with 10.0% labeled data: 0.9619\n",
      "Accuracy with 5.0% labeled data: 0.9506\n",
      "Accuracy with 1.0% labeled data: 0.936\n",
      "Accuracy with 0.1% labeled data: 0.677\n",
      "Accuracy with one sample per class: 0.2825\n"
     ]
    }
   ],
   "source": [
    "# Print results\n",
    "for ratio, accuracy in accuracies.items():\n",
    "    if ratio == 'one_per_class':\n",
    "        print(f\"Accuracy with one sample per class: {accuracy}\")\n",
    "    else:\n",
    "        print(f\"Accuracy with {ratio*100}% labeled data: {accuracy}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "EL2805",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
