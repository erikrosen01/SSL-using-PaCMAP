{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unsupervised learning using pacmac\n",
    "### This file shows the first \"vanilla\" implementation to get a baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torchvision\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from pacmap import PaCMAP\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_preprocess_data():\n",
    "    \"\"\"Loads and preprocesses the MNIST dataset.\"\"\"\n",
    "    train_dataset = torchvision.datasets.MNIST(root='./data', train=True, download=True)\n",
    "    test_dataset = torchvision.datasets.MNIST(root='./data', train=False, download=True)\n",
    "\n",
    "    x_train = train_dataset.data.numpy().astype('float32') / 255.0\n",
    "    y_train = train_dataset.targets.numpy()\n",
    "    x_test = test_dataset.data.numpy().astype('float32') / 255.0\n",
    "    y_test = test_dataset.targets.numpy()\n",
    "\n",
    "    return x_train, y_train, x_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(x_train, y_train, labeled_ratio):\n",
    "    \"\"\"Splits the data into labeled and unlabeled data.\"\"\"\n",
    "    num_labeled = int(labeled_ratio * len(x_train))\n",
    "    x_labeled, x_unlabeled, y_labeled, _ = train_test_split(x_train, y_train, train_size=num_labeled, stratify=y_train, random_state=42)\n",
    "    return x_labeled, x_unlabeled, y_labeled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_pacmap(data, n_components, n_neighbors=10, MN_ratio=0.5, FP_ratio=2.0):\n",
    "    \"\"\"Performs PaCMAP on the data.\"\"\"\n",
    "    pacmap = PaCMAP(n_components=n_components, n_neighbors=n_neighbors, MN_ratio=MN_ratio, FP_ratio=FP_ratio)\n",
    "    return pacmap.fit_transform(data.reshape(data.shape[0], -1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    \"\"\"First neural network model. 28*28 -> 256 -> 128 -> 64.\"\"\"\n",
    "    # do the model below with l2 regularization\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.fc1 = nn.Linear(28 * 28, 256)\n",
    "        self.fc2 = nn.Linear(256, 128)\n",
    "        self.fc3 = nn.Linear(128, 64)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SecondNet(nn.Module):\n",
    "    \"\"\"Second neural network model. 64 -> 32 -> 32 -> 16.\"\"\"\n",
    "    def __init__(self, input_dim=64, hidden_dim=32, output_dim=16):\n",
    "        super(SecondNet, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.fc3 = nn.Linear(hidden_dim, output_dim)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ThirdNet(nn.Module):\n",
    "    \"\"\"Third neural network model. 16 -> 16 -> 16 -> 10.\"\"\"\n",
    "    def __init__(self, input_dim=16, hidden_dim=16, num_classes=10):\n",
    "        super(ThirdNet, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.fc3 = nn.Linear(hidden_dim, num_classes)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, criterion, optimizer, num_epochs, device):\n",
    "    \"\"\"Generic training function for a neural network model.\"\"\"\n",
    "    losses = []\n",
    "    for epoch in tqdm(range(num_epochs)):\n",
    "        model.train()\n",
    "        epoch_loss = 0\n",
    "        for batch_x, batch_y in train_loader:\n",
    "            batch_x, batch_y = batch_x.to(device), batch_y.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(batch_x)\n",
    "            loss = criterion(outputs, batch_y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.item()\n",
    "        losses.append(epoch_loss / len(train_loader))\n",
    "    return losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_first_stage(x_unlabeled, device):\n",
    "    \"\"\"Trains the first neural network.\"\"\"\n",
    "    x_reduced = perform_pacmap(x_unlabeled, n_components=64)\n",
    "    x_train_nn = torch.FloatTensor(x_unlabeled).unsqueeze(1)\n",
    "    y_train_nn = torch.FloatTensor(x_reduced)\n",
    "    train_dataset = TensorDataset(x_train_nn, y_train_nn)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "    model1 = Net().to(device)\n",
    "    criterion1 = nn.MSELoss()\n",
    "    optimizer1 = optim.Adam(model1.parameters())\n",
    "    losses1 = train_model(model1, train_loader, criterion1, optimizer1, num_epochs=10, device=device)\n",
    "    \n",
    "    return model1, x_train_nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_second_stage(model1, x_train_nn, device):\n",
    "    \"\"\"Trains the second neural network.\"\"\"\n",
    "    transformed_unlabeled = model1(x_train_nn.to(device)).detach().cpu().numpy()\n",
    "    x_transformed_16 = perform_pacmap(transformed_unlabeled, n_components=16)\n",
    "    x_train_2 = torch.FloatTensor(transformed_unlabeled)\n",
    "    y_train_2 = torch.FloatTensor(x_transformed_16)\n",
    "    train_dataset_2 = TensorDataset(x_train_2, y_train_2)\n",
    "    train_loader_2 = DataLoader(train_dataset_2, batch_size=32, shuffle=True)\n",
    "\n",
    "    model2 = SecondNet().to(device)\n",
    "    criterion2 = nn.MSELoss()\n",
    "    optimizer2 = optim.Adam(model2.parameters())\n",
    "    losses2 = train_model(model2, train_loader_2, criterion2, optimizer2, num_epochs=10, device=device)\n",
    "    \n",
    "    return model2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_third_stage(model1, model2, x_labeled, y_labeled, device):\n",
    "    \"\"\"Trains the third neural network.\"\"\"\n",
    "    x_labeled_tensor = torch.FloatTensor(x_labeled).unsqueeze(1)\n",
    "    with torch.no_grad():\n",
    "        model1.eval()\n",
    "        model2.eval()\n",
    "        intermediate = model1(x_labeled_tensor.to(device))\n",
    "        processed_labeled = model2(intermediate).cpu().numpy()\n",
    "\n",
    "    x_train_3 = torch.FloatTensor(processed_labeled)\n",
    "    y_train_3 = torch.LongTensor(y_labeled)\n",
    "    train_dataset_3 = TensorDataset(x_train_3, y_train_3)\n",
    "    train_loader_3 = DataLoader(train_dataset_3, batch_size=32, shuffle=True)\n",
    "\n",
    "    model3 = ThirdNet().to(device)\n",
    "    criterion3 = nn.CrossEntropyLoss()\n",
    "    optimizer3 = optim.Adam(model3.parameters())\n",
    "    losses3 = train_model(model3, train_loader_3, criterion3, optimizer3, num_epochs=10, device=device)\n",
    "    \n",
    "    return model3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_and_classify(x_new, model1, model2, model3, device):\n",
    "    \"\"\"Classifies new data using the trained models.\"\"\"\n",
    "    with torch.no_grad():\n",
    "        model1.eval()\n",
    "        model2.eval()\n",
    "        model3.eval()\n",
    "        x_new_tensor = torch.FloatTensor(x_new).unsqueeze(1).to(device)\n",
    "        intermediate = model1(x_new_tensor)\n",
    "        processed = model2(intermediate)\n",
    "        output = model3(processed)\n",
    "        _, predicted = output.max(1)\n",
    "    return predicted.cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_models(model1, model2, model3, x_test, y_test, device):\n",
    "    \"\"\"Evaluates the trained models on the test set.\"\"\"\n",
    "    predicted_classes = process_and_classify(x_test, model1, model2, model3, device)\n",
    "    accuracy = np.mean(predicted_classes == y_test)\n",
    "    print(f\"Accuracy on the test set: {accuracy:.2f}\")\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_evaluate(x_labeled, y_labeled, x_unlabeled, x_test, y_test):\n",
    "    \"\"\"Trains model using unlabeled and labeled data, then evaluates on test-data.\"\"\"\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    \n",
    "    # First stage: PaCMAP data to 64 dimensions, then train a NN with embeddings as targets\n",
    "    model1, x_train_nn = train_first_stage(x_unlabeled, device)\n",
    "\n",
    "    # Second stage: PaCMAP the output of the first NN to 16 dimensions, then train a NN with embeddings as targets\n",
    "    model2 = train_second_stage(model1, x_train_nn, device)\n",
    "\n",
    "    # Third stage: Take the output of the first NN, pass it through the second NN, then train a classifier on the labeled data\n",
    "    model3 = train_third_stage(model1, model2, x_labeled, y_labeled, device)\n",
    "\n",
    "    # Evaluate on test set\n",
    "    accuracy = evaluate_models(model1, model2, model3, x_test, y_test, device)\n",
    "    \n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_evaluate_with_labeled_ratio(labeled_ratio):\n",
    "    \"\"\"Trains and evaluates the model given a ratio of labeled data.\"\"\"\n",
    "    x_train, y_train, x_test, y_test = load_and_preprocess_data()\n",
    "    x_labeled, x_unlabeled, y_labeled = split_data(x_train, y_train, labeled_ratio)\n",
    "    accuracy = train_and_evaluate(x_labeled, y_labeled, x_unlabeled, x_test, y_test)\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labeled_ratios = [0.5, 0.1, 0.05, 0.01, 0.001]\n",
    "accuracies = {}\n",
    "\n",
    "for ratio in labeled_ratios:\n",
    "    print(f\"\\nTraining with {ratio*100}% labeled data:\")\n",
    "    accuracy = train_and_evaluate_with_labeled_ratio(ratio)\n",
    "    accuracies[ratio] = accuracy\n",
    "labeled_ratios = [0.5, 0.1, 0.05, 0.01, 0.001]\n",
    "accuracies_val = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, y_train, x_test, y_test = load_and_preprocess_data()\n",
    "\n",
    "# One sample per class\n",
    "x_labeled_one_per_class = []\n",
    "y_labeled_one_per_class = []\n",
    "x_unlabeled_one_per_class = []\n",
    "for i in range(10):\n",
    "    indices = np.where(y_train == i)[0]\n",
    "    x_labeled_one_per_class.append(x_train[indices[0]])\n",
    "    y_labeled_one_per_class.append(i)\n",
    "    x_unlabeled_one_per_class.extend(x_train[indices[1:]])\n",
    "\n",
    "x_labeled_one_per_class = np.array(x_labeled_one_per_class)\n",
    "y_labeled_one_per_class = np.array(y_labeled_one_per_class)\n",
    "x_unlabeled_one_per_class = np.array(x_unlabeled_one_per_class)\n",
    "\n",
    "print(\"\\nTraining with one sample per class:\")\n",
    "accuracy_one_per_class = train_and_evaluate(x_labeled_one_per_class, y_labeled_one_per_class, x_unlabeled_one_per_class, x_test, y_test)\n",
    "accuracies['one_per_class'] = accuracy_one_per_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the data into labeled and unlabeled sets using different ratios\n",
    "num_labeled_0_5 = int(0.5 * len(x_train))\n",
    "num_labeled_0_1 = int(0.1 * len(x_train))\n",
    "num_labeled_0_05 = int(0.05 * len(x_train))\n",
    "num_labeled_0_01 = int(0.01 * len(x_train))\n",
    "num_labeled_0_001 = int(0.001 * len(x_train))\n",
    "\n",
    "x_labeled_0_5, x_unlabeled_0_5, y_labeled_0_5, _ = train_test_split(x_train, y_train, train_size=num_labeled_0_5, stratify=y_train, random_state=42)\n",
    "x_labeled_0_1, x_unlabeled_0_1, y_labeled_0_1, _ = train_test_split(x_train, y_train, train_size=num_labeled_0_1, stratify=y_train, random_state=42)\n",
    "x_labeled_0_05, x_unlabeled_0_05, y_labeled_0_05, _ = train_test_split(x_train, y_train, train_size=num_labeled_0_05, stratify=y_train, random_state=42)\n",
    "x_labeled_0_01, x_unlabeled_0_01, y_labeled_0_01, _ = train_test_split(x_train, y_train, train_size=num_labeled_0_01, stratify=y_train, random_state=42)\n",
    "x_labeled_0_001, x_unlabeled_0_001, y_labeled_0_001, _ = train_test_split(x_train, y_train, train_size=num_labeled_0_001, stratify=y_train, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a data split where all data is unlabeled except for one sample from each class\n",
    "x_labeled_one_per_class = []\n",
    "y_labeled_one_per_class = []\n",
    "x_unlabeled_one_per_class = []\n",
    "y_unlabeled_one_per_class = []\n",
    "\n",
    "for i in range(10):\n",
    "    indices = np.where(y_train == i)[0]\n",
    "    x_labeled_one_per_class.append(x_train[indices[0]])\n",
    "    y_labeled_one_per_class.append(i)\n",
    "    x_unlabeled_one_per_class.extend(x_train[indices[1:]])\n",
    "    y_unlabeled_one_per_class.extend(y_train[indices[1:]])\n",
    "\n",
    "x_labeled_one_per_class = np.array(x_labeled_one_per_class)\n",
    "y_labeled_one_per_class = np.array(y_labeled_one_per_class)\n",
    "x_unlabeled_one_per_class = np.array(x_unlabeled_one_per_class)\n",
    "\n",
    "print(\"\\nTraining with one sample per class:\")\n",
    "accuracy_one_per_class = train_and_evaluate(x_labeled_one_per_class, y_labeled_one_per_class, x_unlabeled_one_per_class, x_test, y_test)\n",
    "accuracies['one_per_class'] = accuracy_one_per_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy with 50% labeled data: 0.9526\n",
      "Accuracy with 10% labeled data: 0.9602\n",
      "Accuracy with 5% labeled data: 0.9571\n",
      "Accuracy with 1% labeled data: 0.9489\n",
      "Accuracy with one sample per class: 0.2783\n"
     ]
    }
   ],
   "source": [
    "# Print results\n",
    "for ratio, accuracy in accuracies.items():\n",
    "    if ratio == 'one_per_class':\n",
    "        print(f\"Accuracy with one sample per class: {accuracy}\")\n",
    "    else:\n",
    "        print(f\"Accuracy with {ratio*100}% labeled data: {accuracy}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "EL2805",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
